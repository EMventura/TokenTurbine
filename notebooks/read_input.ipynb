{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_jsonl(file_path, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Analyze .jsonl file structure and content\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to .jsonl file\n",
    "        sample_size: Number of documents to sample (use fewer for faster analysis)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Storage for analysis\n",
    "    all_keys = Counter()\n",
    "    key_types = defaultdict(Counter)\n",
    "    text_field_candidates = defaultdict(list)\n",
    "    doc_sizes = []\n",
    "    sample_docs = []\n",
    "    \n",
    "    print(f\"Analyzing {file_path}...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= sample_size:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                doc = json.loads(line)\n",
    "                \n",
    "                # Store first few docs for manual inspection\n",
    "                if i < 5:\n",
    "                    sample_docs.append(doc)\n",
    "                \n",
    "                # Track document size\n",
    "                doc_sizes.append(len(line))\n",
    "                \n",
    "                # Analyze structure\n",
    "                for key, value in doc.items():\n",
    "                    all_keys[key] += 1\n",
    "                    key_types[key][type(value).__name__] += 1\n",
    "                    \n",
    "                    # Identify potential text fields\n",
    "                    if isinstance(value, str) and len(value) > 100:\n",
    "                        text_field_candidates[key].append(len(value))\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Malformed JSON at line {i+1}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    num_docs_analyzed = min(i + 1, sample_size)\n",
    "    \n",
    "    # ===== REPORT RESULTS =====\n",
    "    \n",
    "    print(f\"\\nüìä ANALYZED {num_docs_analyzed} DOCUMENTS\\n\")\n",
    "    \n",
    "    # 1. Show sample documents\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. SAMPLE DOCUMENTS (first 3)\")\n",
    "    print(\"=\" * 60)\n",
    "    for idx, doc in enumerate(sample_docs[:3], 1):\n",
    "        print(f\"\\n--- Document {idx} ---\")\n",
    "        print(json.dumps(doc, indent=2, ensure_ascii=False)[:500])\n",
    "        if len(json.dumps(doc)) > 500:\n",
    "            print(\"... (truncated)\")\n",
    "    \n",
    "    # 2. All fields discovered\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. ALL FIELDS DISCOVERED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Field Name':<30} {'Frequency':<15} {'Type(s)'}\")\n",
    "    print(\"-\" * 60)\n",
    "    for key, count in all_keys.most_common():\n",
    "        types = ', '.join([f\"{t}({c})\" for t, c in key_types[key].most_common()])\n",
    "        percentage = (count / num_docs_analyzed) * 100\n",
    "        print(f\"{key:<30} {count:>6} ({percentage:>5.1f}%)  {types}\")\n",
    "    \n",
    "    # 3. Identify text field\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. TEXT FIELD CANDIDATES\")\n",
    "    print(\"=\" * 60)\n",
    "    if text_field_candidates:\n",
    "        print(f\"{'Field Name':<20} {'Avg Length':<15} {'Min':<10} {'Max':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        for field, lengths in sorted(text_field_candidates.items(), \n",
    "                                     key=lambda x: statistics.mean(x[1]), \n",
    "                                     reverse=True):\n",
    "            avg_len = statistics.mean(lengths)\n",
    "            min_len = min(lengths)\n",
    "            max_len = max(lengths)\n",
    "            print(f\"{field:<20} {avg_len:>10.0f} chars  {min_len:>8}  {max_len:>10}\")\n",
    "        \n",
    "        best_text_field = max(text_field_candidates.items(), \n",
    "                             key=lambda x: statistics.mean(x[1]))[0]\n",
    "        print(f\"\\nüéØ RECOMMENDED TEXT FIELD: '{best_text_field}'\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No string fields longer than 100 characters found!\")\n",
    "    \n",
    "    # 4. Document size analysis\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. DOCUMENT SIZE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    if doc_sizes:\n",
    "        avg_size = statistics.mean(doc_sizes)\n",
    "        median_size = statistics.median(doc_sizes)\n",
    "        min_size = min(doc_sizes)\n",
    "        max_size = max(doc_sizes)\n",
    "        \n",
    "        print(f\"Average document size: {avg_size:,.0f} bytes ({avg_size/1024:.1f} KB)\")\n",
    "        print(f\"Median document size:  {median_size:,.0f} bytes ({median_size/1024:.1f} KB)\")\n",
    "        print(f\"Min document size:     {min_size:,.0f} bytes\")\n",
    "        print(f\"Max document size:     {max_size:,.0f} bytes ({max_size/1024:.1f} KB)\")\n",
    "        \n",
    "        # Batching recommendation\n",
    "        print(\"\\nüì¶ BATCHING RECOMMENDATIONS:\")\n",
    "        if avg_size < 10_000:  # < 10KB\n",
    "            print(\"   - Small documents: Use batch_size=1000-5000\")\n",
    "        elif avg_size < 100_000:  # < 100KB\n",
    "            print(\"   - Medium documents: Use batch_size=100-500\")\n",
    "        else:\n",
    "            print(\"   - Large documents: Use batch_size=10-50\")\n",
    "    \n",
    "    # 5. Metadata summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. METADATA FIELDS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    common_metadata_fields = ['source', 'url', 'domain', 'timestamp', 'date', \n",
    "                             'id', 'author', 'title', 'lang', 'language']\n",
    "    \n",
    "    found_metadata = []\n",
    "    for field in common_metadata_fields:\n",
    "        if field in all_keys:\n",
    "            found_metadata.append(field)\n",
    "    \n",
    "    if found_metadata:\n",
    "        print(\"Found metadata fields:\")\n",
    "        for field in found_metadata:\n",
    "            percentage = (all_keys[field] / num_docs_analyzed) * 100\n",
    "            print(f\"  - {field}: present in {percentage:.1f}% of documents\")\n",
    "    else:\n",
    "        print(\"No standard metadata fields found.\")\n",
    "        print(\"\\nAll fields could be metadata:\")\n",
    "        for key in all_keys:\n",
    "            if key not in text_field_candidates:\n",
    "                print(f\"  - {key}\")\n",
    "    \n",
    "    # 6. Data quality checks\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. DATA QUALITY CHECKS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check for nulls/empty values\n",
    "    empty_text_count = 0\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= sample_size:\n",
    "                break\n",
    "            try:\n",
    "                doc = json.loads(line)\n",
    "                if best_text_field in text_field_candidates:\n",
    "                    if not doc.get(best_text_field, '').strip():\n",
    "                        empty_text_count += 1\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if empty_text_count > 0:\n",
    "        print(f\"‚ö†Ô∏è  {empty_text_count} documents have empty text fields ({empty_text_count/num_docs_analyzed*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"‚úÖ All sampled documents have non-empty text\")\n",
    "    \n",
    "    return {\n",
    "        'text_field': best_text_field if text_field_candidates else None,\n",
    "        'all_fields': list(all_keys.keys()),\n",
    "        'metadata_fields': found_metadata,\n",
    "        'avg_doc_size': statistics.mean(doc_sizes) if doc_sizes else 0,\n",
    "        'num_docs_sampled': num_docs_analyzed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "results = analyze_jsonl('/home/venturae/Downloads/mainpipe_data_v1.jsonl', sample_size=300000)\n",
    "    \n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Text field to use: {results['text_field']}\")\n",
    "print(f\"Metadata fields: {', '.join(results['metadata_fields'])}\")\n",
    "print(f\"Average document size: {results['avg_doc_size']/1024:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
