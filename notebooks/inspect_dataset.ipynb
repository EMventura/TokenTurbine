{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import os\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751369ca",
   "metadata": {},
   "source": [
    "# CONFIGURATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/processed/cleaned_dataset.jsonl'\n",
    "SAMPLE_SIZE = 10000 # Number of samples for expensive checks (Perplexity/Safety)\n",
    "\n",
    "# Load Data\n",
    "if os.path.exists(DATA_PATH):\n",
    "    print(f\"Loading dataset from {DATA_PATH}...\")\n",
    "    # Adjust error handling for large files if needed, but standard read_json is fine for ~500MB\n",
    "    df = pd.read_json(DATA_PATH, lines=True)\n",
    "    print(f\"Loaded {len(df):,} documents.\")\n",
    "else:\n",
    "    # NOTE: This error message is essential if the user hasn't run the pipeline yet\n",
    "    print(f\"ERROR: File not found at {DATA_PATH}. Please ensure the pipeline has been run successfully.\")\n",
    "    # Creating an empty DataFrame to allow the rest of the script to run without crashing, for demonstration purposes\n",
    "    df = pd.DataFrame({'doc_id': [], 'text': [], 'char_count': [], 'word_count': []})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae68b99",
   "metadata": {},
   "source": [
    "# ## 1. Deduplication Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We verify that the `doc_id`s are unique and inspect content uniqueness.\n",
    "if not df.empty:\n",
    "    total_docs = len(df)\n",
    "    unique_ids = df['doc_id'].nunique()\n",
    "    unique_texts = df['text'].nunique()\n",
    "\n",
    "    print(f\"Total Rows: {total_docs:,}\")\n",
    "    print(f\"Unique IDs: {unique_ids:,}\")\n",
    "    print(f\"Unique Texts: {unique_texts:,}\")\n",
    "\n",
    "    if total_docs == unique_texts:\n",
    "        print(\"✅ SUCCESS: Dataset is 100% deduplicated by exact text match.\")\n",
    "    else:\n",
    "        dup_count = total_docs - unique_texts\n",
    "        print(f\"⚠️ WARNING: Found {dup_count} duplicate text contents (This may be expected if only near-deduplication was performed, not exact).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5848f8",
   "metadata": {},
   "source": [
    "# ## 2. Noise & Integrity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7121df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We analyze document lengths and check for HTML artifacts or non-printable characters.\n",
    "\n",
    "if not df.empty:\n",
    "    # 2a. Length Distribution\n",
    "    # Note: If the pipeline already calculates these, use those columns to avoid re-calculating\n",
    "    if 'char_count' not in df.columns:\n",
    "        df['char_length'] = df['text'].str.len()\n",
    "    else:\n",
    "        df['char_length'] = df['char_count']\n",
    "\n",
    "    if 'word_count' not in df.columns:\n",
    "        df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    sns.histplot(df['char_length'], bins=50, log_scale=(True, False), ax=ax[0], color='teal')\n",
    "    ax[0].set_title(\"Character Length Distribution (Log Scale)\")\n",
    "\n",
    "    sns.histplot(df['word_count'], bins=50, log_scale=(True, False), ax=ax[1], color='coral')\n",
    "    ax[1].set_title(\"Word Count Distribution (Log Scale)\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Mean Length: {df['char_length'].mean():.0f} chars\")\n",
    "    print(f\"Min Length: {df['char_length'].min()} chars\")\n",
    "    print(f\"Max Length: {df['char_length'].max()} chars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8375bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. Integrity Checks (Boilerplate & artifacts)\n",
    "\n",
    "    # Regex for common HTML tags left over (should be 0 after cleaning)\n",
    "    html_re = re.compile(r'<[^>]+>')\n",
    "    # Regex for excessive whitespace (should be low due to normalization)\n",
    "    space_re = re.compile(r'\\s{4,}')\n",
    "\n",
    "    sample_check = df.sample(min(len(df), 5000))\n",
    "\n",
    "    html_hits = sample_check['text'].apply(lambda x: bool(html_re.search(x))).sum()\n",
    "    space_hits = sample_check['text'].apply(lambda x: bool(space_re.search(x))).sum()\n",
    "\n",
    "    print(\"\\nIntegrity Scan (Sample 5000):\")\n",
    "    print(f\"- Documents with HTML artifacts: {html_hits} ({html_hits/len(sample_check):.1%})\")\n",
    "    print(f\"- Documents with excessive whitespace: {space_hits} ({space_hits/len(sample_check):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b84c627",
   "metadata": {},
   "source": [
    "# ## 3. Linguistics: Perplexity (PPL) Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a small LM (`distilgpt2` or `gpt2`) to calculate perplexity on a sample. Lower perplexity generally indicates more natural, coherent text.\n",
    "\n",
    "if not df.empty:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_id = \"distilgpt2\"\n",
    "\n",
    "    print(f\"\\nLoading {model_id} on {device} for perplexity calculation...\")\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "\n",
    "    def calculate_perplexity(text, model, tokenizer, device, stride=512):\n",
    "        encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "        max_length = model.config.n_positions\n",
    "        seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "        nlls = []\n",
    "        prev_end_loc = 0\n",
    "        for begin_loc in range(0, seq_len, stride):\n",
    "            end_loc = min(begin_loc + max_length, seq_len)\n",
    "            trg_len = end_loc - prev_end_loc \n",
    "            input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=target_ids)\n",
    "                neg_log_likelihood = outputs.loss\n",
    "\n",
    "            nlls.append(neg_log_likelihood)\n",
    "            prev_end_loc = end_loc\n",
    "            if end_loc == seq_len:\n",
    "                break\n",
    "\n",
    "        if not nlls: return float('nan')\n",
    "        ppl = torch.exp(torch.stack(nlls).mean())\n",
    "        return ppl.item()\n",
    "\n",
    "    # Run on a random sample\n",
    "    ppl_sample = df.sample(min(len(df), 100))['text'].tolist()\n",
    "    ppls = []\n",
    "\n",
    "    print(f\"Calculating perplexity for {len(ppl_sample)} documents...\")\n",
    "    for i, text in enumerate(ppl_sample):\n",
    "        try:\n",
    "            # Truncate slightly to speed up check\n",
    "            ppls.append(calculate_perplexity(text[:2000], model, tokenizer, device))\n",
    "        except Exception as e:\n",
    "            # print(f\"Error calculating PPL for doc {i}: {e}\")\n",
    "            pass\n",
    "\n",
    "    mean_ppl = np.mean(ppls)\n",
    "    print(f\"\\nMean Perplexity: {mean_ppl:.2f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(ppls, kde=True, color='purple')\n",
    "    plt.title(\"Perplexity Distribution (Sample)\")\n",
    "    plt.xlabel(\"Perplexity\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ca848",
   "metadata": {},
   "source": [
    "# ## 4. Safety: PII & Toxicity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589279c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We verify that PII has been redacted (checking for `<EMAIL>` tags vs real emails) and perform a heuristic scan for toxicity.\n",
    "# NOTE: Toxicity check requires 'detoxify' package if implemented fully. We use a PII proxy here.\n",
    "if not df.empty:\n",
    "    # Regex patterns\n",
    "    redaction_tag_re = re.compile(r'<EMAIL>|<IP>|<PHONE>')\n",
    "    email_leak_re = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "\n",
    "    # Check for redaction tags (Evidence that the pipeline worked)\n",
    "    redaction_counts = df['text'].str.count(redaction_tag_re).sum()\n",
    "\n",
    "    # Check for leaks (Emails that were NOT caught)\n",
    "    leaks = df['text'].apply(lambda x: len(email_leak_re.findall(x)))\n",
    "    total_leaks = leaks.sum()\n",
    "\n",
    "    print(\"\\nPII Safety Analysis:\")\n",
    "    print(f\"- Total Redaction Tags Found (<EMAIL>, etc): {redaction_counts:,}\")\n",
    "    print(f\"- Potential Email Leaks found: {total_leaks:,}\")\n",
    "    print(f\"- Documents with leaks: {len(df[leaks > 0]):,}\")\n",
    "\n",
    "    # Inspect a redaction example\n",
    "    redacted_sample = df[df['text'].str.contains(\"<EMAIL>\", na=False)].head(1)\n",
    "    if not redacted_sample.empty:\n",
    "        print(\"\\n--- Redaction Example ---\")\n",
    "        print(redacted_sample.iloc[0]['text'][:300] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45500449",
   "metadata": {},
   "source": [
    "# ## 5. Coverage: Language Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use FastText (same as the pipeline) to verify the output is predominantly English.\n",
    "\n",
    "if not df.empty:\n",
    "    # Download model if not exists locally (reuse pipeline model)\n",
    "    model_path = \"./lid.176.bin\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"\\nDownloading FastText model for verification...\")\n",
    "        # NOTE: This command requires the system to have `wget` or equivalent\n",
    "        try:\n",
    "            os.system(f\"wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin -O {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not download FastText model automatically: {e}\")\n",
    "            print(\"Please download it manually from the URL above.\")\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        ft_model = fasttext.load_model(model_path)\n",
    "\n",
    "        def predict_lang(text):\n",
    "            text = text.replace(\"\\n\", \" \")[:1000]\n",
    "            # FastText requires a list for predict\n",
    "            res = ft_model.predict([text])\n",
    "            return res[0][0][0]\n",
    "\n",
    "        print(\"\\nVerifying language coverage...\")\n",
    "        sample_lang = df.sample(min(len(df), 2000)).copy()\n",
    "        sample_lang['lang'] = sample_lang['text'].apply(predict_lang)\n",
    "\n",
    "        # Remove the '__label__' prefix for cleaner display\n",
    "        sample_lang['lang'] = sample_lang['lang'].str.replace('__label__', '')\n",
    "        lang_dist = sample_lang['lang'].value_counts()\n",
    "        print(lang_dist)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.barplot(x=lang_dist.index, y=lang_dist.values)\n",
    "        plt.title(\"Language Distribution (Post-Pipeline)\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nSkipping Language Distribution check: FastText model not found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
